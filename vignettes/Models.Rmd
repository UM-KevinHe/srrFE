---
title: "Models"
author: "Yubo Shao"
date: <span style="font-style:normal;font-family:'Open Sans'">`r Sys.Date()`</span>
output: 
  rmarkdown::html_vignette:
    self_contained: yes
    mode: selfcontained
vignette: >
  %\VignetteIndexEntry{Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  eval = F, 
  collapse = T, 
  comment = "#>")
```

<style>
body {
  text-align: justify
}
</style>

###  Estimation

The computational tools designed for general-purpose model fitting in generalized linear models, such as those relying on Newton-Raphson and Fisher scoring, fall short in meeting the computational demands as the number of providers increases. This limitation arises due to the substantial escalation in computational cost when incorporating thousands of provider effects into the parameter space. Consequently, the inversion of the Fisher information matrix becomes a computationally burdensome task, posing a significant challenge even for workstations specifically engineered for such purposes. To address this issue, the SerBIN algorithm, proposed by [Wu et al. (2022)](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.9387), leverages the block structure of the Fisher information matrix. This innovative approach substantially reduces the time complexity associated with inverting the high-dimensional matrix, particularly when confronted with the presence of thousands of provider effects.

Consider a binary outcome as an illustration. Let $m$ represent the total number of providers, $n_i$ denote the number of subjects from provider $i$ $(i = 1, \dots, m)$, and $N := \sum_{i = 1}^m n_i$ signify the total number of records. For subject $j$ $(j = 1, \dots, n_i)$ from provider $i$, let $Y_{ij}$ be the outcome variable, and let $X_{ij}$ constitute the vector of risk factors. The likelihood function can then be formulated as follows:
$$l(\gamma, \beta) \propto \sum_{i = 1}^m \sum_{j = 1}^{n_i}\{Y_{ij} \cdot (\gamma_i + X_{ij}\beta) - log(1 + e^{\gamma_i + X_{ij}\beta})\},$$ and the corresponding score function can be expressed as: $$U(\gamma_i) = \sum_{j = 1}^{n_i} \{Y_{ij} - \frac{e^{\gamma_i + X_{ij}\beta}}{1 + e^{\gamma_i + X_{ij}\beta}}\}$$ $$U(\beta_p) = \sum_{i = 1}^m \sum_{j = 1}^{n_i} \{X_{ijp} \cdot (Y_{ij} - \frac{e^{\gamma_i + X_{ij}\beta}}{1 + e^{\gamma_i + X_{ij}\beta}})\}.$$ 
In the current context, the information matrix can be visualized as a $2 \times 2$ block matrix, expressed as:
$$ I(\gamma, \beta) = \begin{pmatrix} I(\gamma) & I(\gamma, \beta)   \\   I(\beta, \gamma) & I(\beta)  \end{pmatrix} \equiv \begin{pmatrix} I_{11} & I_{12} \\ I_{21} & I_{22}  \end{pmatrix}.$$ 
It is important to note that the $I(\gamma)$ block is "large" but diagonal. Consequently, we can compute $I^{-1}(\gamma, \beta)$ as follows: $$I^{-1}(\gamma, \beta) = \begin{pmatrix} I_{11}^{-1} + {J_1}^{T} S^{-1} J_1 & - {J_2}^{T}   \\   - {J_2} &  S^{-1}  \end{pmatrix},$$ where $J_1 = I_{21}{I_{11}}^{-1}$, $S = I_{22} - J_1 I_{12}$, and $J_2 = S^{-1}J_1$.

The bottleneck in SerBIN lies in computing $I_{22}$ (i.e., $I(\beta)$) when the sample size $n$ is exceptionally large. However, each element of $I_{22}$ can be individually computed, allowing for parallel computing to enhance computational efficiency:
$$I(\beta)_{r,c} = \langle X^{(r)}, X^{(c)} \circ \mathbb{P} \rangle,$$ where $\langle a, b \rangle$ denotes the [inner product](https://en.wikipedia.org/wiki/Inner_product_space#:~:text=In%20mathematics%2C%20an%20inner%20product,angle%20brackets%20such%20as%20in) of $a$ and $b$ (i.e., $a^T b$), $\circ$ represents the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (element-wise product), and $\mathbb{P} = \frac{e^{\gamma_i + X_{ij}\beta}}{(1 + e^{\gamma_i + X_{ij}\beta})^2}$. It's worth noting that $I(\beta)$ is symmetric, so there's a need to calculate only the upper triangular entries.

*(To avoid instability for $\gamma$ estimation in our code, we restrict the ${\hat{\gamma}}^{[l]}$ estimates to a specific range during each iteration. The default range is ${\hat{\gamma}_{med}}^{[l-1]} \pm 10$.)*

---

### Standardized Ratio/Rate

In addition to the outputs for covariate coefficients and facility effects, users can use the `SR_output()` function by specifying the standardization method ("direct" or "indirect") for computing facility-level ratio/rate, where:

* **Indirect standardization ratio (ISR)** is computed as follows:

$$ISR_i = \frac{O_i}{E_i},$$ where $O_i = \sum_{j = 1}^{n_i} Y_{ij}$ is the observed number of events for facility $i$, and $E_i$ is the expected number of events for facility $i$, calculated as $E_i = \sum_{j = 1}^{n_i} \frac{e^{\gamma_0 + X_{ij}\beta}}{1 + e^{\gamma_0 + X_{ij}\beta}}$. Here, $\gamma_0$ represents the "population average", defaulting to the median value of all $\hat{\gamma}_i$'s. An $ISR_i >1$ indicates that facility $i$ has worse performance than the population average.

* **Direct standardization ratio (DSR)** is determined by the formula:

$${DSR}_k = \frac{E_k}{O},$$ where $O = \sum_{i = 1}^m \sum_{j = 1}^{n_i} Y_{ij}$, and $E_{k} = \sum_{i = 1}^m \sum_{j = 1}^{n_i} \frac{e^{\hat{\gamma}_k + X_{ij}\beta}}{1 + e^{\hat{\gamma}_k + X_{ij}\beta}}$. 

*(Note: If $\hat{\gamma}_k = \infty$, $E_{k} = N$, resulting in ${DSR}_{k} = \frac{N}{O}$; and if $\hat{\gamma}_k = -\infty$, $E^{(k)} = 0$, leading to ${DSR}_{k} = 0$.)*

Users also have the option to request the function to generate a "standardized rate", which is computed as $ISR_i$ or $DSR_i \times \frac{O}{N}$. 

*(Please be aware that the computed "indirect rate" might surpass 100%. To prevent any confusion, we manually constrain the result within the range of 0% to 100%.)*

---

##  Hypothesis Testing

The `test_fe()` function provides hypothesis testing outcomes to assist users in identifying outlier providers with extreme outcomes. By default, we use the median of all estimated facility effects as the null value (i.e. $H_0: \gamma_i = \hat{\gamma}_{med}$). Users can choose from various testing methods, including "exact.poisbinom" (default), "exact.bootstrap," "score," and "wald":

* **"exact.poisbinom"**: Assume $Y_{ij}|X_{ij} \sim Bernoulli(p_{ij})$. Since within the facility $i$, $p_{ij} = \frac{e^{\gamma_i + X_{ij}\beta}}{1 + e^{\gamma_i + X_{ij}\beta}}$ varies, $O_i | X_i = \sum_{j = 1}^{n_i}Y_{ij}$ will follow the Poisson-binomial distribution. We then calculate the probability of "getting a more extreme number of $O_i$, if $H_0$ is true" based on the Poisson-binomial distribution.

* **"exact.bootstrap"**: The test is conducted in the following steps: (1) Repeat $B =$ 10,000 times (default): Simulate ${Y_{ij}}^{(b)} \sim Bernoulli(\frac{e^{\gamma_0 + X_{ij}\beta}}{1 + e^{\gamma_0 + X_{ij}\beta}})$, and calculate ${Y_{i}}^{(b)} = \sum_{j = 1}^{n_i} {Y_{ij}}^{(b)}$. (2) Calculate the frequency that ${Y_{i}}^{(b)}$ is more extreme than $O_i$. (3) calculate the $p-$value. *("Exact Bootstrap" is consistent to "Exact Poisson-binomial")*

* **"wald"**: When $H_0: \gamma_i = \gamma_0$ is true, $\frac{\hat{\gamma}_i - \gamma_0}{se(\hat{\gamma}_i)} \sim N(0, 1)$. The $se(\hat{\gamma}_i)$ can be computed using the formula mentioned earlier ($I_{11}^{-1} + {J_1}^{T} S^{-1} J_1$) but with only diagonal elements. *(The "Wald test" is not valid for outlying facilities (i.e. $\hat{\gamma}_i = \pm \infty$). "Error message" is added in the code to inform users.)*

* **"score"**: In the standard score test procedure, the first step involves computing the "restricted MLE" of $\hat{\beta}^{(i)}$. Subsequently, calculate $U(\gamma_i)|_{\gamma_i = \gamma_0} = \sum_{j = 1}^{n_i} \{Y_{ij} - \frac{e^{\gamma_0 + X_{ij}\hat{\beta}^{(i)}}}{1 + e^{\gamma_0 + X_{ij}\hat{\beta}^{(i)}}}\}$, and $I(\gamma_i)|_{\gamma_i = \gamma_0} = \sum_{j = 1}^{n_i} \{\frac{e^{\gamma_0 + X_{ij}\hat{\beta}^{(i)}}}{(1 + e^{\gamma_0 + X_{ij}\hat{\beta}^{(i)}})^2}\}$. Under $H_0$, $\frac{U(\gamma_i)|_{\gamma_i = \gamma_0}}{\sqrt{I(\gamma_i)|_{\gamma_i = \gamma_0}}} \sim N(0, 1)$. However, given the potentially high dimensionality of $\gamma$, re-fitting the new restricted model for each provider effect test could be challenging. Therefore, instead of using $\hat{\beta}^{(i)}$, we solely employ the $\hat{\beta}$ estimated by the full model.


---

##  Confidence Interval

The choices for calculating confidence intervals of $\gamma_i$ include the following methods:

* **"exact"**: Based on the Poisson binomial distribution of $O_i|X_i$. We numerically find the range of $\gamma_0$ such that $P_{exact} \geq 0.05$, which involves locating the roots of an equation.

* **"wald"**: Directly convert $\hat{\gamma}_i \pm 1.96 \times se(\hat{\gamma}_i)$, which is invalid for outlying facilities. 

* **"score"**: Find the range of $\gamma_0$ such that $\frac{U(\gamma_i)|_{\gamma_i = \gamma_0}}{\sqrt{I(\gamma_i)|_{\gamma_i = \gamma_0}}} \in (-1.96, 1.96)$.


We can also derive the confidence interval for $ISR_i$ or $DSR_i$ based on the confidence interval for $\gamma_i$. 

* $ISR_i$: Given that the score equation provides $O_i = \sum_{j = 1}^{n_i} \frac{e^{\hat{\gamma}_i + X_{ij}\hat{\beta}}}{1 + e^{\hat{\gamma}_i + X_{ij}\hat{\beta}}}$, the CI of $ISR_i$ can be calculated as:

$$CI_{ISR_i} = (\frac{\sum_{j = 1}^{n_i} \frac{e^{\gamma_{i,L} + X_{ij}\hat{\beta}}}{1 + e^{\gamma_{i,L} + X_{ij}\hat{\beta}}}}{E_i}, \frac{\sum_{j = 1}^{n_i} \frac{e^{\gamma_{i,U} + X_{ij}\hat{\beta}}}{1 + e^{\gamma_{i,U} + X_{ij}\hat{\beta}}}}{E_i}).$$ Here, $\gamma_{i,L}$ and $\gamma_{i,U}$ are the lower and upper bounds of $\gamma_i$. *(If $CI(\gamma_i)$ covers $\gamma_0$, then $CI_{ISR_i}$ will cover 1.)*

* $DSR_i$: Considering $SRR^{(0)} = \frac{\sum_{i = 1}^m \sum_{j = 1}^{n_i} \frac{e^{{\gamma}_0 + X_{ij}\beta}}{1 + e^{{\gamma}_0 + X_{ij}\beta}}}{\sum_{i = 1}^m O_i}.$ The CI of $DSR_i$ can be calculated as:

$$CI_{DSR_i} = (\frac{\sum_{i = 1}^m \sum_{j = 1}^{n_i} \frac{e^{{\gamma}_{i,L} + X_{ij}\beta}}{1 + e^{{\gamma}_{i,L} + X_{ij}\beta}}}{\sum_{i = 1}^m O_i}, \frac{\sum_{i = 1}^m \sum_{j = 1}^{n_i} \frac{e^{{\gamma}_{i,U} + X_{ij}\beta}}{1 + e^{{\gamma}_{i,U} + X_{ij}\beta}}}{\sum_{i = 1}^m O_i}).$$ *(If $CI(\gamma_i)$ covers $\gamma_0$, then $CI_{ISR_i}$ will cover $ISR_0 = \frac{\sum_{i = 1}^m \sum_{j = 1}^{n_i} \frac{e^{{\gamma}_0 + X_{ij}\beta}}{1 + e^{{\gamma}_0 + X_{ij}\beta}}}{\sum_{i = 1}^m O_i}$.)*
